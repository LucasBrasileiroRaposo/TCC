{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizando experimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISTRAL E LLAMAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "     ---------------------------------------- 0.0/131.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 131.1/131.1 kB 3.9 MB/s eta 0:00:00\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB ? eta 0:00:00\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.5 MB 7.7 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.6/8.5 MB 7.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.0/8.5 MB 8.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.5/8.5 MB 8.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.1/8.5 MB 9.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.8/8.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.7/8.5 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.8/8.5 MB 13.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.0/8.5 MB 15.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.2/8.5 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.5/8.5 MB 17.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.5/8.5 MB 16.5 MB/s eta 0:00:00\n",
      "Downloading torch-2.2.1-cp311-cp311-win_amd64.whl (198.6 MB)\n",
      "   ---------------------------------------- 0.0/198.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/198.6 MB 35.9 MB/s eta 0:00:06\n",
      "    --------------------------------------- 3.1/198.6 MB 32.7 MB/s eta 0:00:06\n",
      "    --------------------------------------- 4.4/198.6 MB 31.7 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 5.8/198.6 MB 31.0 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 7.2/198.6 MB 30.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 8.7/198.6 MB 30.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 9.4/198.6 MB 30.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 10.0/198.6 MB 26.7 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 11.5/198.6 MB 26.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 12.9/198.6 MB 27.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 14.3/198.6 MB 26.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 15.9/198.6 MB 27.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 17.3/198.6 MB 28.5 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 18.7/198.6 MB 27.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 20.2/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 21.6/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 23.1/198.6 MB 31.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 24.6/198.6 MB 32.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 26.2/198.6 MB 32.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 27.5/198.6 MB 32.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 29.0/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 30.4/198.6 MB 31.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 32.0/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 33.7/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 35.3/198.6 MB 32.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 36.7/198.6 MB 32.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 38.3/198.6 MB 32.7 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 39.9/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 41.6/198.6 MB 34.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 43.3/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 45.1/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 46.9/198.6 MB 36.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 48.4/198.6 MB 36.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 50.0/198.6 MB 38.6 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 50.9/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 52.1/198.6 MB 32.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 53.0/198.6 MB 31.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 53.8/198.6 MB 28.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 54.6/198.6 MB 26.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 55.4/198.6 MB 25.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 56.3/198.6 MB 23.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 57.2/198.6 MB 22.6 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 58.1/198.6 MB 21.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 59.1/198.6 MB 20.5 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 60.0/198.6 MB 19.8 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 61.0/198.6 MB 19.2 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 61.9/198.6 MB 19.2 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 62.9/198.6 MB 19.3 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 63.9/198.6 MB 19.9 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 64.8/198.6 MB 19.9 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 65.8/198.6 MB 20.5 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 66.9/198.6 MB 20.5 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 67.9/198.6 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 68.9/198.6 MB 21.1 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 69.9/198.6 MB 21.1 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 70.9/198.6 MB 21.1 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 72.0/198.6 MB 21.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 73.0/198.6 MB 21.8 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 73.6/198.6 MB 21.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 74.0/198.6 MB 19.8 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 74.5/198.6 MB 19.3 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 75.1/198.6 MB 19.3 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 75.8/198.6 MB 18.2 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 76.2/198.6 MB 16.8 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 76.9/198.6 MB 16.4 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 77.7/198.6 MB 16.4 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 78.8/198.6 MB 16.4 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 79.9/198.6 MB 16.4 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 80.9/198.6 MB 16.4 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 82.0/198.6 MB 16.4 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 83.0/198.6 MB 16.4 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 83.4/198.6 MB 16.4 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 83.8/198.6 MB 15.6 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 84.7/198.6 MB 16.4 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 85.3/198.6 MB 16.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 86.2/198.6 MB 17.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 87.2/198.6 MB 18.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 88.1/198.6 MB 19.3 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 89.1/198.6 MB 18.7 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 90.0/198.6 MB 18.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 91.2/198.6 MB 18.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 92.4/198.6 MB 19.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 93.3/198.6 MB 18.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 94.4/198.6 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 95.6/198.6 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 96.8/198.6 MB 23.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 97.9/198.6 MB 23.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 99.2/198.6 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 100.4/198.6 MB 25.1 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 101.7/198.6 MB 25.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 102.5/198.6 MB 24.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 103.6/198.6 MB 25.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 104.1/198.6 MB 23.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 104.9/198.6 MB 22.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 106.2/198.6 MB 23.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 107.6/198.6 MB 23.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 109.0/198.6 MB 23.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 110.3/198.6 MB 23.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 111.4/198.6 MB 24.2 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 112.8/198.6 MB 25.1 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 114.0/198.6 MB 25.1 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 115.4/198.6 MB 28.5 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 116.9/198.6 MB 29.8 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 118.4/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 119.8/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 121.3/198.6 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 122.8/198.6 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 124.4/198.6 MB 32.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 125.8/198.6 MB 32.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 127.3/198.6 MB 32.8 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 128.9/198.6 MB 32.8 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 130.4/198.6 MB 32.8 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 131.8/198.6 MB 32.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 133.4/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 134.9/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 136.4/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 137.8/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 139.4/198.6 MB 32.8 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 141.0/198.6 MB 32.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 142.7/198.6 MB 32.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 144.3/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 146.0/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 147.4/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 149.0/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 150.6/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 152.2/198.6 MB 34.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 153.8/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 155.3/198.6 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 156.9/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 158.7/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 160.3/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 161.6/198.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 162.8/198.6 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 164.0/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 165.3/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 166.5/198.6 MB 29.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 167.7/198.6 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 169.0/198.6 MB 27.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 170.3/198.6 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 171.6/198.6 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 172.8/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 174.1/198.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 175.5/198.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 176.6/198.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 177.8/198.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 179.1/198.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 180.5/198.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.9/198.6 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 183.2/198.6 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 184.7/198.6 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 186.1/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 187.5/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 188.8/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 190.2/198.6 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 191.7/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 193.2/198.6 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  194.0/198.6 MB 28.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.4/198.6 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  196.4/198.6 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  197.4/198.6 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.4/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 198.6/198.6 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 330.1/330.1 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "   ---------------------------------------- 0.0/280.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 280.0/280.0 kB 16.9 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "   ---------------------------------------- 0.0/170.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 170.9/170.9 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "   ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 144.7/144.7 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.5/269.5 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp311-none-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.6/269.6 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.0/2.2 MB 31.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 25.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 23.2 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.2/133.2 kB ? eta 0:00:00\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "   ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.7 MB 31.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.1/5.7 MB 26.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.3/5.7 MB 26.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.2/5.7 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.3/5.7 MB 24.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.7/5.7 MB 21.5 MB/s eta 0:00:00\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 163.8/163.8 kB ? eta 0:00:00\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 35.1 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, typing-extensions, tqdm, sympy, safetensors, regex, pyyaml, networkx, MarkupSafe, idna, fsspec, filelock, certifi, requests, jinja2, torch, huggingface-hub, tokenizers, accelerate, transformers\n",
      "Successfully installed MarkupSafe-2.1.5 accelerate-0.27.2 certifi-2024.2.2 filelock-3.13.1 fsspec-2024.2.0 huggingface-hub-0.20.3 idna-3.6 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 sympy-1.12 tokenizers-0.15.2 torch-2.2.1 tqdm-4.66.2 transformers-4.38.1 typing-extensions-4.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers torch huggingface-hub accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Lucas Brasileiro\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli login --token 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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBRz\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-hf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.63it/s]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# LLM taks document-question-answering\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",  # LLM task\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sequences[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     22\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI liked \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBreaking Bad\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBand of Brothers\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Do you have any recommendations of other shows I might like?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mget_llama_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36mget_llama_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_llama_response\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Generate a response from the Llama model.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m        None: Prints the model's response.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m \u001b[43mllama_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sequences[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1190\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1193\u001b[0m         )\n\u001b[0;32m   1194\u001b[0m     )\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1202\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1203\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1204\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1589\u001b[0m     )\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1617\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1168\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1165\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1181\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1008\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    997\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    998\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    999\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         cache_position,\n\u001b[0;32m   1006\u001b[0m     )\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1008\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1018\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:749\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    748\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    752\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:236\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    234\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Lucas Brasileiro\\Documents\\Projetos\\TCC\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_llama_response(prompt: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a response from the Llama model.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The user's input/question for the model.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the model's response.\n",
    "    \"\"\"\n",
    "    sequences = llama_pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=256,\n",
    "    )\n",
    "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
    "\n",
    "\n",
    "\n",
    "prompt = 'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n'\n",
    "get_llama_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chegou aqui\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaModel\n",
    "import torch\n",
    "# model_name_or_path = \"kashif/stack-llama-2\" #path/to/your/model/or/name/on/hub\n",
    "device = \"cpu\" # or \"cuda\" if you have a GPU\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "print(\"chegou aqui\")\n",
    "model = LlamaModel.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=10)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submetendo as questões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTS_PATH = '../TCC/csvs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n"
     ]
    }
   ],
   "source": [
    "years = os.listdir(f'{TESTS_PATH}')\n",
    "txt = \"\"\n",
    "for test in [2023]:\n",
    "    print(test)\n",
    "    test_df = pd.read_csv(f'{TESTS_PATH}/enem_{test}_questions.csv')\n",
    "\n",
    "    for questions in range(test_df.shape[0]): \n",
    "        txt += f\"\\nusando esse enunciado:\\n {test_df.loc[questions]['body']} \\n escolha entre as opções abaixo a que acredite ser a resposta correta (indique apenas qual a letra correta, com apenas 1 único caractere de resposta):\\n {test_df.loc[questions]['alternatives']}\"\n",
    "\n",
    "with open('experiment01.txt','w',encoding='utf-8') as f:\n",
    "    f.write(txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapeando os gabaritos retornados dos modelos UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "CSV_PATH = \"../TCC/csvs\"\n",
    "EXPERIMENTS_PATH = '../TCC/experiments'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_answers(answers_key_txt):\n",
    "    answers_key_lines = answers_key_txt.split('\\n')\n",
    "    \n",
    "    keys = []\n",
    "    for e in answers_key_lines:\n",
    "        if(e and len(e.strip()) <= 3) or (e.strip() == \"Anulado\"):\n",
    "            keys.append(e.strip())\n",
    "            \n",
    "\n",
    "    answers_map = {}\n",
    "    right_answers = []\n",
    "    keys.reverse()\n",
    "        \n",
    "    for item in keys:\n",
    "        if(item.isnumeric()):\n",
    "            if (int(item) < 10):\n",
    "                item = '0' + item\n",
    "            if(len(right_answers) > 1):\n",
    "                right_answers.reverse()\n",
    "                answers_map[item + \"I\"] = right_answers[0]\n",
    "                answers_map[item + \"E\"] = right_answers[1]   \n",
    "                right_answers = []\n",
    "                continue\n",
    "            answers_map[item] = right_answers[0] if len(right_answers) > 0 else 'Anulado'\n",
    "            right_answers = []\n",
    "        else:\n",
    "            if(item == \"Anulado\"):\n",
    "                right_answers.append(item)\n",
    "            elif(item != ' ' and item != '' and item.isalpha()):\n",
    "                right_answers.append(item)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "             \n",
    "    # Exibindo o resultado\n",
    "    return answers_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_models_answers(answers_map, year, experiment_name):\n",
    "    df = pd.read_csv(f'{CSV_PATH}/enem_{year}_questions.csv')\n",
    "    df[experiment_name] = df['id'].map(answers_map)\n",
    "    print(len(answers_map))\n",
    "    print(answers_map)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n",
      "{'180': 'E', '179': 'C', '178': 'C', '177': 'A', '176': 'B', '175': 'D', '174': 'D', '173': 'E', '172': 'D', '171': 'C', '170': 'C', '169': 'B', '168': 'C', '167': 'C', '166': 'D', '165': 'C', '164': 'A', '163': 'E', '162': 'D', '161': 'D', '160': 'D', '159': 'B', '158': 'B', '157': 'D', '156': 'D', '155': 'E', '154': 'A', '153': 'A', '152': 'E', '151': 'C', '150': 'B', '149': 'C', '148': 'B', '147': 'B', '146': 'B', '145': 'E', '144': 'A', '143': 'B', '142': 'D', '141': 'C', '140': 'B', '139': 'B', '138': 'A', '137': 'B', '136': 'C', '135': 'D', '134': 'B', '133': 'A', '132': 'E', '131': 'B', '130': 'D', '129': 'A', '128': 'D', '127': 'B', '126': 'A', '125': 'B', '124': 'B', '123': 'D', '122': 'A', '121': 'A', '120': 'B', '119': 'D', '118': 'E', '117': 'C', '116': 'A', '115': 'B', '114': 'E', '113': 'D', '112': 'B', '111': 'B', '110': 'B', '109': 'C', '108': 'C', '107': 'C', '106': 'E', '105': 'E', '104': 'E', '103': 'B', '102': 'E', '101': 'C', '100': 'C', '99': 'E', '98': 'E', '97': 'D', '96': 'E', '95': 'E', '94': 'D', '93': 'D', '92': 'B', '91': 'C', '90': 'E', '89': 'D', '88': 'B', '87': 'A', '86': 'D', '85': 'C', '84': 'D', '83': 'A', '82': 'B', '81': 'E', '80': 'C', '79': 'E', '78': 'C', '77': 'B', '76': 'E', '75': 'A', '74': 'B', '73': 'E', '72': 'E', '71': 'A', '70': 'B', '69': 'A', '68': 'E', '67': 'B', '66': 'D', '65': 'A', '64': 'B', '63': 'A', '62': 'C', '61': 'E', '60': 'E', '59': 'C', '58': 'A', '57': 'E', '56': 'A', '55': 'D', '54': 'C', '53': 'A', '52': 'D', '51': 'B', '50': 'C', '49': 'D', '48': 'C', '47': 'B', '46': 'A', '45': 'C', '44': 'C', '43': 'C', '42': 'C', '41': 'D', '40': 'D', '39': 'E', '38': 'A', '37': 'B', '36': 'B', '35': 'B', '34': 'D', '33': 'A', '32': 'D', '31': 'E', '30': 'D', '29': 'D', '28': 'C', '27': 'C', '26': 'C', '25': 'A', '24': 'D', '23': 'B', '22': 'E', '21': 'A', '20': 'B', '19': 'D', '18': 'A', '17': 'B', '16': 'A', '15': 'C', '14': 'A', '13': 'A', '12': 'E', '11': 'B', '10': 'B', '09': 'C', '08': 'E', '07': 'A', '06': 'A', '05I': 'A', '05E': 'E', '04I': 'B', '04E': 'C', '03I': 'B', '03E': 'A', '02I': 'D', '02E': 'A', '01I': 'B', '01E': 'B'}\n"
     ]
    }
   ],
   "source": [
    "years = os.listdir(f'{EXPERIMENTS_PATH}')\n",
    "for year in years:\n",
    "    \n",
    "    experiments = os.listdir(f'{EXPERIMENTS_PATH}/{year}')\n",
    "    \n",
    "    for experiment in experiments:\n",
    "        \n",
    "        experiment_name = experiment.split('.txt')[0]\n",
    "        \n",
    "        with open(f'{EXPERIMENTS_PATH}/{year}/{experiment}', 'r', encoding='utf-8') as exam_str:\n",
    "            content = exam_str.read()\n",
    "            answers_map = map_answers(content)\n",
    "            df_with_answers = aggregate_models_answers(answers_map, year,experiment_name)\n",
    "            # display(df_with_answers)\n",
    "            df_with_answers.to_csv(f'{EXPERIMENTS_PATH}/{year}/enem_{year}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
