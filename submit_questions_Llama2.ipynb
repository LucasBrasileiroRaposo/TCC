{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (1.13.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from openai) (2.6.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucas brasileiro\\documents\\projetos\\tcc\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questão = \"\"\"QUESTÃO 126  \n",
    "Um garoto comprou vários abacates na feira, mas \n",
    "descobriu que eles não estavam maduros o suficiente \n",
    "para serem consumidos. Sua mãe recomendou que ele \n",
    "colocasse os abacates em um recipiente fechado, pois \n",
    "isso aceleraria seu amadurecimento. Com certa dúvida, o \n",
    "garoto realizou esta experiência: colocou alguns abacates \n",
    "no recipiente e deixou os demais em uma fruteira aberta. \n",
    "Surpreendendo-se, ele percebeu que os frutos que estavam \n",
    "no recipiente fechado amadureceram mais rapidamente.\n",
    " A aceleração desse processo é causada por\"\"\"\n",
    "alternativas = \"\"\"\n",
    "A acúmulo de gás etileno.\n",
    "B redução da umidade do ar.\n",
    "C aumento da concentração de CO2.\n",
    "D diminuição da intensidade luminosa.\n",
    "E isolamento do contato com O2 atmosférico.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": null,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"max_token\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"A resposta correta é: A acúmulo de gás etileno. \\n\\nExplanation: \\nO\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1709908271,\n",
      "  \"model\": \"llama-70b-chat\",\n",
      "  \"object\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 24,\n",
      "    \"prompt_tokens\": 294,\n",
      "    \"total_tokens\": 318\n",
      "  }\n",
      "}\n",
      "A resposta correta é: A acúmulo de gás etileno. \n",
      "\n",
      "Explanation: \n",
      "O\n"
     ]
    }
   ],
   "source": [
    "# This example is the new way to use the OpenAI lib for python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "api_key = \"LL-8TEmrzbYgkmK420Q2gMNmvzoNL2JhY7rNyp62M0qzqyuUvwfvMTIHVQxJI6TeVn9\",\n",
    "base_url = \"https://api.llama-api.com\"\n",
    ")\n",
    "\n",
    "conteudo = \"Você deve responder perguntas em português sobre diversos temas. Suas respostas devem sem compostas com APENAS a letra da alternativa correta, não retorne mais nada para o user. Answer only with the character of the correct alternative. A,B,C,D or E.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-70b-chat\",\n",
    "    max_tokens= 30,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that should answer questions in Portuguese about various topics. Completing the phrase: A resposta correta é:. You must not answer more than the letter of the alternative\"},\n",
    "        {\"role\": \"user\", \"content\": f\"com base nesse enunciado: {questão} \\n responda qual a letra da alternativa correta, entre as opções a seguir: \\n {alternativas} \\n a alternativa correta é:\"}\n",
    "    ],\n",
    "    \n",
    ")\n",
    "\n",
    "#print(response)\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "context = \"You are going to answer questions in Portuguese about various topics. Your answers must be composed with ONLY the letter of the correct alternative, do not return anything else to the user, such as explanation and text, only the letter. Completing the phrase:  A resposta correta é: \"\n",
    "\n",
    "def submit_questions(prompt):    \n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key = \"LL-8TEmrzbYgkmK420Q2gMNmvzoNL2JhY7rNyp62M0qzqyuUvwfvMTIHVQxJI6TeVn9\",\n",
    "        base_url = \"https://api.llama-api.com\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-70b-chat\",\n",
    "        max_tokens= 30,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f'{context}'},\n",
    "            {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "        ],\n",
    "        \n",
    "    )\n",
    "    # print(response.choices[0].message.content.replace('\\n', ''))\n",
    "\n",
    "    if response.choices[0].message.content == \"\" or response.choices[0].message.content == None or len(response.choices[0].message.content) < 3:\n",
    "        raise Exception(\"A resposta está vazia\")\n",
    "    \n",
    "    if ( not \" A\" in response.choices[0].message.content) and ( not \"B\" in response.choices[0].message.content) and ( not \"C\" in response.choices[0].message.content) and ( not \"D\" in response.choices[0].message.content) and ( not \"E\" in response.choices[0].message.content):\n",
    "        raise Exception(\"A resposta não contém a letra da alternativa correta\")\n",
    "   \n",
    "    return response.choices[0].message.content.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVS_PATH = '../TCC/csvs'\n",
    "EXPERIMENTS_PATH = '../TCC/experiments'\n",
    "EXPERIMENT_NUMBER = 1\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "years = [2023]\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    enem_csv = pd.read_csv(f'{CSVS_PATH}/enem_{year}_questions.csv').copy()\n",
    "    model_answers = ''    \n",
    "    problematic_questions = []\n",
    "    problematic_outputs = []\n",
    "    \n",
    "    for index, row in enem_csv.iterrows():\n",
    "        \n",
    "        question = row['body']\n",
    "        alternatives = row['alternatives']\n",
    "        prompt = f'usando como base esse enunciado: {question} \\n responda qual a alternativa correta, entre as opções a seguir: \\n {alternatives} \\n a alternativa correta é:'\n",
    "        teste = ''\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            teste = submit_questions(prompt)\n",
    "            print(teste)\n",
    "            \n",
    "        except:\n",
    "            problematic_questions.append(row['id'])\n",
    "            \n",
    "            with open(f'{EXPERIMENTS_PATH}/{EXPERIMENT_NUMBER}/{year}/Llama2_{year}_answers.txt', 'w+',encoding='utf-8') as f:\n",
    "                f.write(model_answers)\n",
    "                \n",
    "        model_answers += f'{row[\"id\"]}\\n{teste}\\n'\n",
    "        \n",
    "    \n",
    "    model_answers += f'\\nQuestões problemáticas: {problematic_questions}'\n",
    "    model_answers += f'\\nSaídas problemáticas: {problematic_outputs}'\n",
    "    \n",
    "    with open(f'{EXPERIMENTS_PATH}/{EXPERIMENT_NUMBER}/{year}/Llama2_{year}_answers.txt', 'w+',encoding='utf-8') as f:\n",
    "        f.write(model_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
