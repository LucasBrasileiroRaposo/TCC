{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llamaapi in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (0.1.36)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ronpdf (C:\\Users\\eulal\\IdeaProjects\\TCC\\venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ronpdf (C:\\Users\\eulal\\IdeaProjects\\TCC\\venv\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.5 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from llamaapi) (3.9.3)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.6 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from llamaapi) (1.5.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.27.1 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from llamaapi) (2.31.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install llamaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"created\": 1709754600,\n",
      "  \"model\": \"llama-13b-chat\",\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 184,\n",
      "    \"completion_tokens\": 28,\n",
      "    \"total_tokens\": 212\n",
      "  },\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"get_current_weather\",\n",
      "          \"arguments\": {\n",
      "            \"location\": \"Boston\",\n",
      "            \"days\": 5,\n",
      "            \"unit\": \"celsius\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"function_call\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from llamaapi import LlamaAPI\n",
    "\n",
    "# Initialize the SDK\n",
    "llama = LlamaAPI(\"LL-8TEmrzbYgkmK420Q2gMNmvzoNL2JhY7rNyp62M0qzqyuUvwfvMTIHVQxJI6TeVn9\")\n",
    "\n",
    "# Build the API request\n",
    "api_request_json = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"},\n",
    "    ],\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"days\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"for how many days ahead you wants the forecast\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\", \"days\"],\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"function_call\": \"get_current_weather\",\n",
    "}\n",
    "\n",
    "# Execute the Request\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from openai) (2.6.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\eulal\\ideaprojects\\tcc\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ronpdf (C:\\Users\\eulal\\IdeaProjects\\TCC\\venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ronpdf (C:\\Users\\eulal\\IdeaProjects\\TCC\\venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "questão = \"\"\"QUESTÃO 150  \n",
    "O Índice de Massa Corporal (IMC) é largamente \n",
    "utilizado há cerca de 200 anos, mas esse cálculo \n",
    "representa muito mais a corpulência que a adiposidade, \n",
    "uma vez que indivíduos musculosos e obesos podem \n",
    "apresentar o mesmo IMC. Uma nova pesquisa aponta \n",
    "o Índice de Adiposidade Corporal (IAC) como uma \n",
    "alternativa mais fidedigna para quantificar a gordura \n",
    "corporal, utilizando a medida do quadril e a altura. A \n",
    "descrição abaixo mostra como calcular essas medidas, \n",
    "sabendo-se que, em mulheres, a adiposidade normal \n",
    "está entre 19% e 26%.\n",
    "O velho IMC\n",
    "Índice de Massa Corporal (IMC) \n",
    " \n",
    "IMC = massa (kg) / [altura × altura (m)]\n",
    "O novo IAC\n",
    "Índice de Adiposidade Corporal (IAC) \n",
    " \n",
    "% de Gordura Corporal = circunferência do \n",
    "quadril (cm) / [altura (m) × raiz quadrada da altura (m)] - 18\n",
    "Uma jovem com IMC = 20 kg/m², 100 cm de circunferência \n",
    "dos quadris e 60 kg de massa corpórea resolveu \n",
    "averiguar seu IAC. Para se enquadrar aos níveis de \n",
    "normalidade de gordura corporal, a atitude adequada \n",
    "que essa jovem deve ter diante da nova medida é\n",
    "(Use 3^(1/2) = 1,7 e 1,7^(1/2) = 1,3 )\"\"\"\n",
    "\n",
    "alternativas = \"\"\"A reduzir seu excesso de gordura em cerca de 1%.\n",
    "B reduzir seu excesso de gordura em cerca de 27%.\n",
    "C manter seus níveis atuais de gordura.\n",
    "D aumentar seu nível de gordura em cerca de 1%.\n",
    "E aumentar seu nível de gordura em cerca de 27%.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "questão = \"\"\"QUESTÃO 96  \n",
    "Avaliação de substâncias genotóxicas\n",
    "Micronúcleos em células tumorais: biologia e implicações para a tumorigênese\n",
    "O ensaio de micronúcleos é um teste de avaliação de \n",
    "genotoxicidade que associa a presença de micronúcleos \n",
    "(pequenos núcleos que aparecem próximo aos núcleos \n",
    "das células) com lesões genéticas. Os micronúcleos são \n",
    "fragmentos de DNA encapsulados, provenientes do fuso \n",
    "mitótico durante a divisão celular.\n",
    "Os micronúcleos se originam dos(as)\"\"\"\n",
    "alternativas = \"\"\"\n",
    "A nucléolos.\n",
    "B lisossomos.\n",
    "C ribossomos.\n",
    "D mitocôndrias.\n",
    "E cromossomos.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": null,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"max_token\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"A alternativa correta é A, nu\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1709778937,\n",
      "  \"model\": \"llama-70b-chat\",\n",
      "  \"object\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 10,\n",
      "    \"prompt_tokens\": 228,\n",
      "    \"total_tokens\": 238\n",
      "  }\n",
      "}\n",
      "A alternativa correta é A, nu\n"
     ]
    }
   ],
   "source": [
    "# This example is the new way to use the OpenAI lib for python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "api_key = \"LL-8TEmrzbYgkmK420Q2gMNmvzoNL2JhY7rNyp62M0qzqyuUvwfvMTIHVQxJI6TeVn9\",\n",
    "base_url = \"https://api.llama-api.com\"\n",
    ")\n",
    "\n",
    "conteudo = \"Você deve responder perguntas em português sobre diversos temas. Suas respostas devem sem compostas com APENAS a letra da alternativa correta, não retorne mais nada para o user. Answer only with the character of the correct alternative. A,B,C,D or E.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-70b-chat\",\n",
    "    max_tokens= 10,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that should answer questions in Portuguese about various topics. \"},\n",
    "        {\"role\": \"user\", \"content\": f\"com base nesse enunciado: {questão} \\n responda qual a letra da alternativa correta, entre as opções a seguir: \\n {alternativas} \\n a alternativa correta é:\"}\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"You are going to answer questions in Portuguese about various topics. Your answers should be composed with ONLY the letter of the correct alternative, do not return anything else to the user, such as explanation and text, only the letter. Answer only with the character of the correct alternative, completing the following phrase with one of the alternatives: A, B, C, D or E.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "api_key = \"LL-8TEmrzbYgkmK420Q2gMNmvzoNL2JhY7rNyp62M0qzqyuUvwfvMTIHVQxJI6TeVn9\",\n",
    "base_url = \"https://api.llama-api.com\"\n",
    ")\n",
    "\n",
    "def submit_questions(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-70b-chat\",\n",
    "        max_tokens= 20,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "        \n",
    "    )\n",
    "    if response.choices[0].message.content == \"\" or response.choices[0].message.content == None:\n",
    "        raise Exception(\"A resposta está vazia\")\n",
    "    if ( not \"A\" in response.choiches[0].message.content) and ( not \"B\" in response.choiches[0].message.content) and ( not \"C\" in response.choiches[0].message.content) and ( not \"D\" in response.choiches[0].message.content) and ( not \"E\" in response.choiches[0].message.content):\n",
    "        raise Exception(\"A resposta não contém a letra da alternativa correta\")\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVS_PATH = '../TCC/csvs'\n",
    "EXPERIMENTS_PATH = '../TCC/experiments'\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "years = [2023]\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    enem_csv = pd.read_csv(f'{CSVS_PATH}/enem_{year}_questions.csv').copy()\n",
    "    model_answers = ''    \n",
    "    problematic_questions = []\n",
    "    problematic_outputs = []\n",
    "    \n",
    "    for index, row in enem_csv.iterrows():\n",
    "        \n",
    "        question = row['body']\n",
    "        alternatives = row['alternatives']\n",
    "        prompt = f'usando como base esse enunciado: {question} \\n responda qual a alternativa correta, entre as opções a seguir: \\n {alternatives} \\n a alternativa correta é:'\n",
    "        teste = ''\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            teste = submit_questions(prompt)\n",
    "            print(teste)\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            problematic_questions.append(row['id'])\n",
    "            \n",
    "            with open(f'{EXPERIMENTS_PATH}/{year}/LLama2_{year}_answers.txt', 'w+',encoding='utf-8') as f:\n",
    "                f.write(model_answers)\n",
    "        \n",
    "        print(teste[0])\n",
    "        \n",
    "        if(teste[0] not in ['A', 'B', 'C', 'D', 'E']):\n",
    "            problematic_outputs.append(row['id'])\n",
    "                \n",
    "        model_answers += f'{row[\"id\"]}\\n{teste}\\n'\n",
    "    \n",
    "    model_answers += f'\\nQuestões problemáticas: {problematic_questions}'\n",
    "    model_answers += f'\\nSaídas problemáticas: {problematic_outputs}'\n",
    "    \n",
    "    with open(f'{EXPERIMENTS_PATH}/{year}/GEMINI_{year}_answers.txt', 'w+',encoding='utf-8') as f:\n",
    "        f.write(model_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
